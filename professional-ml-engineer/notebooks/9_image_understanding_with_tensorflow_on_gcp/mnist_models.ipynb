{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "electronic-power",
   "metadata": {},
   "source": [
    "# MNIST Image Classification with TensorFlow on Cloud AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-christian",
   "metadata": {},
   "source": [
    "This lab demonstrates how to implement different image models on MNIST using the `tf.keras` API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-shepherd",
   "metadata": {},
   "source": [
    "**Learning objectives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-front",
   "metadata": {},
   "source": [
    "1. Understand how to build a Dense Neural Network (DNN) for image classification\n",
    "2. Understand how to use dropout for image classification\n",
    "3. Understand how to use Convolutional Neural Networks (CNN)\n",
    "4. Know how to deploy and use an image classification model using Cloud AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "PROJECT = \"your-project-id-here\"\n",
    "BUCKET = \"your-bucket-id-here\"\n",
    "REGION = \"us-central1\"\n",
    "MODEL_TYPE = \"cnn\"\n",
    "\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"MODEL_TYPE\"] = MODEL_TYPE\n",
    "os.environ[\"TFVERSION\"] = \"2.1\"\n",
    "os.environ[\"IMAGE_URI\"] = os.path.join(\"gcr.io\", PROJECT, \"mnist_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-inspection",
   "metadata": {},
   "source": [
    "## Building a dynamic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-budapest",
   "metadata": {},
   "source": [
    "Model code needs to be packaged as a Python module in order to run it on Cloud AI Platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-surprise",
   "metadata": {},
   "source": [
    "The boilerplate structure for this module has already been set up in the `mnist/learned` folder. The module lives in the sub-folder `trainer` and is designated as a Python package with the empty `__init__.py` (`mnist/trainer/__init__.py`) file. It still needs the model and a trainer to run it, so let's make them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-heath",
   "metadata": {},
   "source": [
    "Let's start with the trainer file first. This file parses command line arguments to feed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mnist_models/trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from . import model\n",
    "\n",
    "def _parse_arguments(argv):\n",
    "    \"\"\"Parses command-line arguments\"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        \"--model_type\",\n",
    "        help=\"Which model type to use\",\n",
    "        type=str,\n",
    "        default=\"linear\")\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        help=\"The number of epochs to train on\",\n",
    "        type=int,\n",
    "        default=10)\n",
    "    parser.add_argument(\n",
    "        \"--steps_per_epoch\",\n",
    "        help=\"The number of steps per epoch to train on\",\n",
    "        type=int,\n",
    "        default=100)\n",
    "    parser.add_argument(\n",
    "        \"--job-dir\",\n",
    "        help=\"Directory where to save the given model\",\n",
    "        type=str,\n",
    "        default=\"mnist/\")\n",
    "    return parser.parse_known_args(argv)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Parses command line arguments and kicks off model training\"\"\"\n",
    "    args = _parse_arguments(sys.argv[1:])[0]\n",
    "    \n",
    "    # Configure path for hyperparameter tuning\n",
    "    trial_id = json.loads(\n",
    "        os.environ.get(\"TF_CONFIG\", \"{}\")).get(\"task\", {}).get(\"trial\", \"\")\n",
    "    output_path = args.job_dir if not trial_id else args.job_dir + \"/\"\n",
    "    \n",
    "    model_layers = model.get_layers(args.model_type)\n",
    "    image_model = model.build_model(model_layers, args.job_dir)\n",
    "    model_history = model.train_and_evaluate(\n",
    "        image_model, args.epochs, args.steps_per_epoch, args.job_dir)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-apollo",
   "metadata": {},
   "source": [
    "Next, let's group non-model functions into a `util` file to keep the model file simple. We'll copy over the `scale` and `load_dataset` from the `MNIST_linear` lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mnist_models/trainer/util.py\n",
    "import tensorflow as tf\n",
    "\n",
    "def scale(image, label):\n",
    "    \"\"\"Scales images from a 0-255 int range to a 0-1 float range\"\"\"\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "    image = tf.expand_dims(image, -1)\n",
    "    return image, label\n",
    "\n",
    "def load_dataset(data, training=True, buffer_size=5000, batch_size=100, nclasses=10):\n",
    "    \"\"\"Loads MNIST data set into a tf.data.Dataset\"\"\"\n",
    "    (x_train, y_train), (x_test, y_test) = data\n",
    "    x = x_train if training else x_test\n",
    "    y = y_train if training else y_test\n",
    "    # One-hot encode the classes\n",
    "    y = tf.keras.utils.to_categorical(y, nclasses)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataste = dataset.map(scale).batch(batch_size)\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(buffer_size).repeat()\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-phase",
   "metadata": {},
   "source": [
    "Finally, let's code the models! The `tf.keras` API accepts an array of `layers` into a `model` object, so we can create a dictionary of layers based on the different model types we want to use. The below file has two functions: `get_layers` and `create_and_train_model`. We will build the structure of our model in `get_layers`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-chuck",
   "metadata": {},
   "source": [
    "These models progressively build on each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mnist_models/trainer/model.py\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D, Softmax\n",
    "\n",
    "from . import util\n",
    "\n",
    "# Image variables\n",
    "WIDTH = 28\n",
    "HEIGHT = 28\n",
    "\n",
    "\n",
    "def get_layers(\n",
    "    model_type,\n",
    "    nclasses=10,\n",
    "    hidden_layer_1_neurons=400,\n",
    "    hidden_layer_2_neurons=100,\n",
    "    dropout_rate=0.25,\n",
    "    num_filters_1=64,\n",
    "    kernel_size_1=3,\n",
    "    pooling_size_1=2,\n",
    "    num_filters_2=32,\n",
    "    kernel_size_2=3,\n",
    "    pooling_size_2=2):\n",
    "    \"\"\"Constructs layers for a Keras model based on a dict of model types\"\"\"\n",
    "    model_layers = {\n",
    "        \"linear\": [\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(nclasses),\n",
    "            tf.keras.layers.Softmax()\n",
    "        ],\n",
    "        \"dnn\": [\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(hidden_layer_1_neurons, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(hidden_layer_2_neurons, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(nclasses),\n",
    "            tf.keras.layers.Softmax()\n",
    "        ],\n",
    "        \"dnn_dropout\": [\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(hidden_layer_1_neurons, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(hidden_layer_2_neurons, activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(nclasses),\n",
    "            tf.keras.layers.Softmax()\n",
    "        ],\n",
    "        \"cnn\": [\n",
    "            tf.keras.layers.Conv2D(num_filters_1, kernel_size=kernel_size_1, activation=\"relu\",\n",
    "                                   input_shape=(WIDTH, HEIGHT, 1)),\n",
    "            tf.keras.layers.MaxPooling2D(pooling_size_1),\n",
    "            tf.keras.layers.Conv2D(num_filters_2, kernel_size=kernel_size_2, activation=\"relu\"),\n",
    "            tf.keras.layers.MaxPooling2D(pooling_size_2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(hidden_layer_1_neurons, activation=\"relu\"),\n",
    "            tf.keras.layers.Dense(hidden_layer_2_neurons, activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(dropout_rate),\n",
    "            tf.keras.layers.Dense(nclasses),\n",
    "            tf.keras.layers.Softmax()\n",
    "        ]\n",
    "        \n",
    "    }\n",
    "    return model_layers[model_type]\n",
    "\n",
    "def build_model(layers, output_dir):\n",
    "    \"\"\"Compiles Keras model for image classification\"\"\"\n",
    "    model = tf.keras.Sequential(layers)\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(model, num_epochs, steps_per_epoch, output_dir):\n",
    "    \"\"\"Compiles Keras model and loads data into it for training\"\"\"\n",
    "    mnist = tf.keras.datasets.mnist.load_data()\n",
    "    train_data = util.load_dataset(mnist)\n",
    "    validation_data = util.load_dataset(mnist, training=False)\n",
    "    \n",
    "    callbacks = []\n",
    "    if output_dir:\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=output_dir)\n",
    "        callbacks = [tensorboard_callback]\n",
    "        \n",
    "    history = model.fit(\n",
    "        train_data,\n",
    "        validation_data=validation_data,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        verbose=2,\n",
    "        callbacks=callbacks)\n",
    "    \n",
    "    if output_dir:\n",
    "        export_path = os.path.join(output_dir, \"keras_export\")\n",
    "        model.save(export_path, save_format=\"tf\")\n",
    "        \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-homeless",
   "metadata": {},
   "source": [
    "## Local training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-halloween",
   "metadata": {},
   "source": [
    "With everything set up, let's run the code locally to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m mnist_models.trainer.test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surprised-defensive",
   "metadata": {},
   "source": [
    "Our model is working well locally. Let's test it on Cloud AI Platform. We can run it as a Python module locally first using the command line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-recording",
   "metadata": {},
   "source": [
    "The below cell transfers some of our variables to the command line as well as it creates a job directory including timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "model_type = \"cnn\"\n",
    "\n",
    "os.environ[\"MODEL_TYPE\"] = model_type\n",
    "os.environ[\"JOB_DIR\"] = \"mnist_models/models/{}_{}/\".format(\n",
    "    model_type, current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-potential",
   "metadata": {},
   "source": [
    "The cell below runs local version of the code. The `epochs` and `steps_per_epoch` flags can be changed to run for longer or shorter, as defined in our `mnist_models/trainer/task.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python3 -m mnist_models.trainer.task \\\n",
    "    --job-dir=$JOB_DIR \\\n",
    "    --epochs=5 \\\n",
    "    --steps_per_epoch=50 \\\n",
    "    --model_type=$MODEL_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-somerset",
   "metadata": {},
   "source": [
    "## Training on the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-census",
   "metadata": {},
   "source": [
    "Since we're using an unreleased version of TensorFlow on Cloud AI Platform, we can instead use a [Deep Learning Container](https://cloud.google.com/ai-platform/deep-learning-containers/docs/overview) to take advantage of libraries and apps not normally packaged with AI Platform. Below is a simple Dockerfile which copies our code to be used in a TensorFlow 2 environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biblical-denver",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mnist_models/Dockerfile\n",
    "FROM gcr.io/deeplearning-platform-release/tf2-cpu\n",
    "COPY mnist_models/trainer /mnist_models/trainer\n",
    "ENTRYPOINT [\"python3\", \"-m\", \"mnist_models.trainer.task\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-designation",
   "metadata": {},
   "source": [
    "The below command builds the image and ships it off to Google Cloud so it can be used on Cloud AI Platform. Cloud Build must be enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -f mnist_models/Dockerfile -t $IMAGE_URI ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-roulette",
   "metadata": {},
   "source": [
    "Finally we can kickoff the [Cloud AI Platform training job](https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training). We can pass in our docker image using the `master-image-uri` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "model_type = \"cnn\"\n",
    "\n",
    "os.environ[\"MODEL_TYPE\"] = model_type\n",
    "os.environ[\"JOB_DIR\"] = \"gs://{}/mnist_{}_{}/\".format(\n",
    "    BUCKET, model_type, current_time)\n",
    "os.environ[\"JOB_NAME\"] = \"mnist_{}_{}\".format(\n",
    "    model_type, current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo $JOB_DIR $REGION $JOB_NAME\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "    --staging-buckets=gs://$BUCKET \\\n",
    "    --region=$REGION \\\n",
    "    --master-image-uri=$IMAGE_URI \\\n",
    "    --scale-tier=BASIC_GPU \\\n",
    "    --job-dir=$JOB_DIR \\\n",
    "    -- \\\n",
    "    --model_type=$MODEL_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-elite",
   "metadata": {},
   "source": [
    "## Deploying and predicting with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-scottish",
   "metadata": {},
   "source": [
    "Once you have a model you're proud of, let's deploy it! All we need to do is give AI Platform the location of the model. Below code uses the Keras export path of the previous job, but `${JOB_DIR}keras_export/` can always be changed to a different path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"mnist\"\n",
    "MODEL_VERSION=${MODEL_TYPE}\n",
    "MODEL_LOCATION=${JOB_DIR}keras_export/\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "gcloud ai-platform versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "gcloud ai-platform models delete ${MODEL_NAME}\n",
    "gcloud config set ai_platform/region global\n",
    "gcloud ai-platform models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ai-platform versions create ${MODEL_VERSION} \\\n",
    "    --model ${MODEL_NAME}\n",
    "    --origin ${MODEL_LOCATION}\n",
    "    --framework tensorflow\n",
    "    --runtime-version=2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-action",
   "metadata": {},
   "source": [
    "To predict with our model, let's take one of the example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-layer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, codecs\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist_models.trainer import util\n",
    "\n",
    "HEIGHT = 28\n",
    "WIDTH = 28\n",
    "IMGNO = 12\n",
    "\n",
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "(x_train, y_train), (x_test, y_test) = mnist\n",
    "test_image = x_test[IMGNO]\n",
    "\n",
    "jsondata = test_image.reshape(HEIGHT, WIDTH, 1).tolist()\n",
    "json.dump(jsondata, codecs.open(\"test.json\", \"w\", encoding=\"utf-8\"))\n",
    "plt.imshow(test_image.reshape(HEIGHT, WIDTH));"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
