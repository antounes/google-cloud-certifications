{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "civic-thailand",
   "metadata": {},
   "source": [
    "# Using pre-trained embedding with Tensorflow Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-paris",
   "metadata": {},
   "source": [
    "**Learning objectives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-colombia",
   "metadata": {},
   "source": [
    "1. How to instantiate a Tensorflow Hub module\n",
    "2. How to find pretrained Tensorflow Hub module\n",
    "3. How to use a pre-trained Tensorflow Hub text module to generate sentence vectors\n",
    "4. How to incorporate a pre-trained Tensorflow Hub module into a Keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-triple",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-difficulty",
   "metadata": {},
   "source": [
    "In this notebook, we will experiment text models to recognise the probable source (Github, Tech-Crunch, or The New York Times) of the titles we have in the title dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-handle",
   "metadata": {},
   "source": [
    "First, we will load and pre-process the texts and labels so that they are suitable to be fed to sequential Keras models with first layer being Tensorflow Hub pre-trained modules. Thanks to this first layer, we won't need to tokenise and integerise the text before passing it to our models. The pre-trained layer will take care of that for us, and consume directly raw text. However, we will still have to one-hot encode each of the 3 classes into a 3 dimensional basis vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-person",
   "metadata": {},
   "source": [
    "Then we will build, train and compare simple models starting with different pre-trained Tensorflow Hub layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-death",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo chown -R jupyter:jupyter /home/jupyter/trainin-data-analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user google-cloud-bigquery==1.25.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-farming",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-partition",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alien-postage",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"cloud-training-demos\" # Replace with your PROJECT\n",
    "BUCKET = PROJECT # defaults to PROJECT\n",
    "REGION = \"us-central1\" # Replace with your REGION\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-patch",
   "metadata": {},
   "source": [
    "## Create a dataset from BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-exclusion",
   "metadata": {},
   "source": [
    "Hackernews headlines are available as a BigQuery public dataset. The dataset contains all headlines from the sites inception in October 2006 until October 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-kingston",
   "metadata": {},
   "source": [
    "Here is a sample of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "\n",
    "SELECT\n",
    "    url, title, score\n",
    "FROM\n",
    "    `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "    LENGTH(title) > 10\n",
    "    AND score > 10\n",
    "    AND LENGTH(url) > 0\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-cover",
   "metadata": {},
   "source": [
    "Let's do some regular expression parsing in BigQuery to get the source of the newspaper article from the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adverse-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "\n",
    "SELECT\n",
    "    ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, \".*://(.[^/]+)/\"), \".\"))[OFFSET(1)] AS source,\n",
    "    COUNT(title) AS num_articles\n",
    "FROM\n",
    "    `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "    REGEXP_CONTAINS(REGEXP_EXTRACT(url, \".*://(.[^/]+)/\"), \".com$\")\n",
    "    AND LENGTH(title > 10)\n",
    "GROUP BY\n",
    "    source\n",
    "ORDER BY num_articles DESC\n",
    "LIMIT 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-correction",
   "metadata": {},
   "source": [
    "Now that we have good parsing of the URL to get the source, let's put together a dataset of source and titles. This will be our labeled dataset for ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-portugal",
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = \".*://(.[^/]+)/\"\n",
    "\n",
    "sub_query = \"\"\"\n",
    "SELECT\n",
    "    title,\n",
    "    ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, '{0}'), '.'))[OFFSET(1)] AS source\n",
    "FROM\n",
    "    `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "    REGEXP_CONTAINS(REGEXP_EXTRACT(url, '{0}'), '.com$')\n",
    "    AND LENGTH(title) > 10\n",
    "\"\"\".format(regex)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    LOWER(REGEXP_REPLACE(title, '[^a-zA-Z0-9 $.-]', ' ')) AS title,\n",
    "    source\n",
    "FROM\n",
    "    ({sub_query})\n",
    "WHERE\n",
    "    (source = \"github\" OR source = \"nytimes\" OR source = \"techcrunch\")\n",
    "\"\"\".format(sub_query=sub_query)\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "particular-mounting",
   "metadata": {},
   "source": [
    "For ML training, we usually need to split our dataset into training and evaluation datasets (and perhaps an independent test dataset if we are going to do model or feature selection based on the evaluation dataset). AutoML however figures out on its own how to create these splits, so we won't need to do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adapted-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "bq = bigquery.Client(project=PROJECT)\n",
    "title_dataset = bq.query(query).to_dataframe()\n",
    "title_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-newspaper",
   "metadata": {},
   "source": [
    "AutoML for text classification requires that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-constitutional",
   "metadata": {},
   "source": [
    "- the dataset to be in csv form with\n",
    "- the first column to be the texts to classify or a GCS path to the text\n",
    "- the last column to be the text labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-embassy",
   "metadata": {},
   "source": [
    "The dataset we pulled from BigQuery satisfies these requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-sterling",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The full dataset contains {n} titles\".format(n=len(title_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-estonia",
   "metadata": {},
   "source": [
    "Let's make sure we have roughly the same number of labels for each of our three labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-marijuana",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_dataset[\"source\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-respect",
   "metadata": {},
   "source": [
    "Finally we will save our data, which is currently in-memory, to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-values",
   "metadata": {},
   "source": [
    "We will create a csv file containing the full dataset and another containing only 1000 articles for development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrow-wireless",
   "metadata": {},
   "source": [
    "**Note**: It may take a long time to train AutoML on the full dataset, so we recommend to use the sample dataset for the purpose of learning the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"./data/\"\n",
    "\n",
    "if not os.path.exists(DATADIR):\n",
    "    os.makedirs(DATADIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_DATASET_NAME = \"titles_full.csv\"\n",
    "FULL_DATASET_PATH = os.path.join(DATADIR, FULL_DATASET_NAME)\n",
    "\n",
    "# Let's shuffle the data before writing it to disk\n",
    "title_dataset = title_dataset.sample(n=len(title_dataset))\n",
    "\n",
    "title_dataset.to_csv(\n",
    "    FULL_DATASET_PATH, header=False, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-dover",
   "metadata": {},
   "source": [
    "Now let's sample 1000 articles from the full dataset and make sure we have enough examples for each label in our sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-advocacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_title_dataset = title_dataset.sample(n=1000)\n",
    "sample_title_dataset[\"source\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-surname",
   "metadata": {},
   "source": [
    "Let's write the sample dataset to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-closure",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_DATASET_NAME = \"titles_sample.csv\"\n",
    "SAMPLE_DATASET_PATH = os.path.join(DATADIR, SAMPLE_DATASET_NAME)\n",
    "\n",
    "sample_title_dataset.to_csv(\n",
    "    SAMPLE_DATASET_PATH, header=False, index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-drawing",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_title_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-burden",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "from tensorflow_hub import KerasLayer\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-decline",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-nurse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-jumping",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-dubai",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-uganda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-palestine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-interface",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-linux",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-token",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-belief",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
