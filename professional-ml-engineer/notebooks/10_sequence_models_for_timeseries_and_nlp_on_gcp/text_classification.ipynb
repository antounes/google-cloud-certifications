{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "computational-proof",
   "metadata": {},
   "source": [
    "# Text classification using TensorFlow/Keras on AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-review",
   "metadata": {},
   "source": [
    "This notebook illustrates:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-tsunami",
   "metadata": {},
   "source": [
    "1. Creating datasets for AI Platform using BigQuery\n",
    "2. Creating a text classification model using the Estimator API with a Keras model\n",
    "3. Training on Cloud AI Platform\n",
    "4. Rerun with pre-trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo chown -R jupyter:jupyter /home/jupyter/training-data-analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user google-cloud-bigquery=1.25.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-college",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = \"cloud-training-demos-ml\"\n",
    "PROJECT = \"cloud-training-demos\"\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"2.1\"\n",
    "\n",
    "if \"COLAB_GPU\" is os.environ: # this is always set on Colab, the value is 0 or 1 depending on whether a GPU is attached\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    # download \"sidecar files\" since on Colab, this notebook will be on Drive\n",
    "    !rm -rf txtclsmodel\n",
    "    !git clone --depth 1 https://github.com/GoogleCloudPlatform/training-data-analyst\n",
    "    !mv training-data-analyst/courses/machine_learning/deepdive/09_sequence/txtclsmodel/ .\n",
    "    !rm -rf training-data-analyst\n",
    "    # downgrade TensorFlow to the version this notebook has been tested with\n",
    "    !pip install --upgrade tensorflow=$TFVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-provincial",
   "metadata": {},
   "source": [
    "We will look at the titles of articles and figure out whether the article came from the New York Times, TechCrunch or GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-galaxy",
   "metadata": {},
   "source": [
    "We will use [hacker news](https://news.ycombinator.com/) as our data source. It is an aggregator that displays tech related headlines from various sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pregnant-graph",
   "metadata": {},
   "source": [
    "## Creating dataset from BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-choir",
   "metadata": {},
   "source": [
    "Hacker news headlines are available as a BigQuery public dataset. The [dataset](https://bigquery.cloud.google.com/table/bigquery-public-data:hacker_news.stories?tab=details) contains all headlines from the sites inception in October 2006 until October 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-moldova",
   "metadata": {},
   "source": [
    "Here is a sample of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strategic-settle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-electron",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-approach",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-excess",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-necklace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-flexibility",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-substance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-percentage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-cover",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
