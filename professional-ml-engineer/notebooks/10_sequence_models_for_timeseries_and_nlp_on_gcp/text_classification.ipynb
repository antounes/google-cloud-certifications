{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sixth-interim",
   "metadata": {},
   "source": [
    "# Text classification using TensorFlow/Keras on AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "orange-quebec",
   "metadata": {},
   "source": [
    "This notebook illustrates:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-island",
   "metadata": {},
   "source": [
    "1. Creating datasets for AI Platform using BigQuery\n",
    "2. Creating a text classification model using the Estimator API with a Keras model\n",
    "3. Training on Cloud AI Platform\n",
    "4. Rerun with pre-trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo chown -R jupyter:jupyter /home/jupyter/training-data-analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-relationship",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user google-cloud-bigquery=1.25.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-river",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = \"cloud-training-demos-ml\"\n",
    "PROJECT = \"cloud-training-demos\"\n",
    "REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"TFVERSION\"] = \"2.1\"\n",
    "\n",
    "if \"COLAB_GPU\" is os.environ: # this is always set on Colab, the value is 0 or 1 depending on whether a GPU is attached\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    # download \"sidecar files\" since on Colab, this notebook will be on Drive\n",
    "    !rm -rf txtclsmodel\n",
    "    !git clone --depth 1 https://github.com/GoogleCloudPlatform/training-data-analyst\n",
    "    !mv training-data-analyst/courses/machine_learning/deepdive/09_sequence/txtclsmodel/ .\n",
    "    !rm -rf training-data-analyst\n",
    "    # downgrade TensorFlow to the version this notebook has been tested with\n",
    "    !pip install --upgrade tensorflow=$TFVERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-wallpaper",
   "metadata": {},
   "source": [
    "We will look at the titles of articles and figure out whether the article came from the New York Times, TechCrunch or GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-builder",
   "metadata": {},
   "source": [
    "We will use [hacker news](https://news.ycombinator.com/) as our data source. It is an aggregator that displays tech related headlines from various sources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-thesaurus",
   "metadata": {},
   "source": [
    "## Creating dataset from BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-central",
   "metadata": {},
   "source": [
    "Hacker news headlines are available as a BigQuery public dataset. The [dataset](https://bigquery.cloud.google.com/table/bigquery-public-data:hacker_news.stories?tab=details) contains all headlines from the sites inception in October 2006 until October 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-allah",
   "metadata": {},
   "source": [
    "Here is a sample of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext google.cloud.bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-circumstances",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "SELECT\n",
    "    url, title, score\n",
    "FROM\n",
    "    `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "    LENGTH(title) > 0\n",
    "    AND score > 10\n",
    "    AND LENGTH(url) > 0\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-circle",
   "metadata": {},
   "source": [
    "Let's do some regular expression parsing in BigQuery to get the source of the newspaper article from the URL. For example, if the URL is http://mobile.nytimes.com/..., I want to be left with _nytimes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "SELECT\n",
    "    ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, \".*://(.[^/]+)/\", \".\"))[OFFSET(1)]) AS source,\n",
    "    COUNT(title) AS num_articles\n",
    "FROM\n",
    "    `bigquery-public-data.hacker_news.stories`\n",
    "WHERE\n",
    "    REGEXP_CONTAINS(REGEXP_EXTRACT(url, \".*://(.[^/]+)/\"), \".com$\")\n",
    "    AND LENGTH(title) > 10\n",
    "GROUP BY\n",
    "    source\n",
    "ORDER BY num_articles DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amateur-substance",
   "metadata": {},
   "source": [
    "Now that we have good parsing of the URL to get the source, let's put together a dataset of source and titles. This will be our labeled dataset for Cloud AI Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "bq = bigquery.Client(project=PROJECT)\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT source, LOWER(REGEXP_REPLACE(title, \"[^a-zA-Z0-9 $.-]\", \" \")) AS title FROM\n",
    "    (SELECT\n",
    "        ARRAY_REVERSE(SPLIT(REGEXP_EXTRACT(url, \".*://(.[^/]+)/\"), \".\"))[OFFSET(1)] AS source,\n",
    "        title\n",
    "    FROM\n",
    "        `bigquery-public-data.hacker_news.stories`\n",
    "    WHERE\n",
    "        REGEXP_CONTAINS(REGEXP_EXTRACT(url, \".*://(.[^/]+)/\"), \".com$\")\n",
    "        AND LENGTH(title) > 10\n",
    "    )\n",
    "WHERE (source = \"github\" OR source = \"nytimes\" OR source = \"techcrunch\")\n",
    "\"\"\"\n",
    "\n",
    "df = bq.query(query + \"LIMIT 5\").to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-integration",
   "metadata": {},
   "source": [
    "For ML training, we will need to split our dataset into training and evaluation datasets (and perhaps an independent test dataset if we are going to do model or feature selection based on the evaluation dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-teaching",
   "metadata": {},
   "source": [
    "A simple, repeatable way to do this is to use the hash of a well-distributed column in our data (see [O'Reilly repeatable sampling](https://www.oreilly.com/learning/repeatable-sampling-of-data-sets-in-bigquery-for-machine-learning))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = bq.query(query + \" AND ABS(MOD(FARM_FINGERPRINT(title), 4)) > 0\").to_dataframe()\n",
    "evaldf = bq.query(query + \" AND ABS(MOD(FARM_FINGERPRINT(title), 4)) = 0\").to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-chambers",
   "metadata": {},
   "source": [
    "Below we can see that roughly 75 % of the data is used for training, and 25 % for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-testing",
   "metadata": {},
   "source": [
    "We can also see that within each dataset, the classes are roughly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf[\"source\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alert-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaldf[\"source\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-smart",
   "metadata": {},
   "source": [
    "Finally, we'll save our data, which is currently in-memory, to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "DATADIR = \"data/txtcls\"\n",
    "shutil.rmtree(DATADIR, ignore_errors=True)\n",
    "os.makedirs(DATADIR)\n",
    "traindf.to_csv(os.path.join(DATADIR, \"train.tsv\"), header=False, index=False, encoding=\"utf-8\", sep=\"\\t\")\n",
    "evaldf.to_csv(os.path.joint(DATADIR, \"eval.tsv\"), header=False, index=False, encoding=\"utf-8\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -3 data/txtcls/train.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-charleston",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l data/txtcls/*.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-scientist",
   "metadata": {},
   "source": [
    "## TensorFlow/Keras code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-primary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-subsection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-theater",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-accuracy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-idaho",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-render",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-projection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pharmaceutical-coast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-delaware",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-outside",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
