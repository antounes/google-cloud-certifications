{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "subtle-board",
   "metadata": {},
   "source": [
    "# An RNN model for temperature data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-proposition",
   "metadata": {},
   "source": [
    "This time we will be working with real data: daily (Tmin, Tmax) temperature series from 36 weather stations spanning 50 years. It is to be noted that a pretty good predictor model already exists for temperatures: the average of temperatures on the same day of the year in N previous years. It is not clear if RNNs can do better but we will see how far they can go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.15.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, \"temperatures/utils/\") # so Python can find the utils_ modules\n",
    "import utils_batching\n",
    "import utils_args\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.lib.io import file_io as gfile\n",
    "print(\"Tensorflow version: \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import utils_prettystyle\n",
    "import utils_display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-eugene",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-planner",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "DOWNLOAD_DIR=temperatures/data\n",
    "mkdir $DOWNLOAD_DIR\n",
    "gsutil -m cp gs://cloud-training-demos/courses/machine_learning/deepdive/09_sequence/temperatures/* $DOWNLOAD_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-square",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-affiliate",
   "metadata": {},
   "source": [
    "N_FORWARD = 1: works but model struggles to predict from some positions\n",
    "\n",
    "N_FORWARD = 4: better but still bad occasionally\n",
    "\n",
    "N_FORWARD = 8: works perfectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCHS = 5 # number of times the model sees all the data during training\n",
    "\n",
    "N_FORWARD = 8 # train the network to predict N in advance (traditionally 1)\n",
    "RESAMPLE_BY = 5 # averaging period in days (training on daily data is too much)\n",
    "RNN_CELLSIZE = 128 # size of the RNN cells\n",
    "N_LAYERS = 2 # number of stacked RNN cells\n",
    "SEQLEN = 128 # unrolled sequence length\n",
    "BATCHSIZE = 64 # mini-batch size\n",
    "DROPOUT_KEEP = 0.7 # probability of neurons not being dropped (should be between 0.5 and 1)\n",
    "ACTIVATION = tf.nn.tanh # activation function for GRU cells (tf.nn.relu or tf.nn.tanh)\n",
    "\n",
    "JOB_DIR = \"checkpoints\"\n",
    "DATA_DIR = \"temperatures/data\"\n",
    "\n",
    "# potentially override some settings from command-line arguments\n",
    "if __name__ == \"__main__\":\n",
    "    JOB_DIR, DATA_DIR = utils_args.read_args1(JOB_DIR, DATA_DIR)\n",
    "    \n",
    "ALL_FILEPATTERN = DATA_DIR + \"/*.csv\" # pattern matches all 1666 files\n",
    "EVAL_FILEPATTERN = DATA_DIR + \"/USC000*2.csv\" # pattern matches 8 files\n",
    "# pattern USW*.csv -> 298 files, pattern USW*0.csv -> 28 files\n",
    "print(\"Reading data from '{}'. \\nWriting checkpoints to '{}'\".format(DATA_DIR, JOB_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-botswana",
   "metadata": {},
   "source": [
    "## Temperature data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-british",
   "metadata": {},
   "source": [
    "This is what our temperature dataset looks like: sequences of daily (Tmin, Tmax) from 1960 to 2010. They have been cleaned up and eventual missing values have been filled by interpolation. Interpolated regions of the dataset are marked in red on the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_filenames = gfile.get_matching_files(ALL_FILEPATTERN)\n",
    "eval_filenames = gfile.get_matching_files(EVAL_FILEPATTERN)\n",
    "train_filenames = list(set(all_filenames) - set(eval_filenames))\n",
    "\n",
    "# By default, this utility function loads all the files and places data\n",
    "# from them as-is in an array, one file per line. Later, we will use it\n",
    "# to shape the dataset as needed for training\n",
    "ite = utils_batching.rnn_multistation_sampling_temperature_sequencer(eval_filenames)\n",
    "evtemps, _, evdates, _, _ = next(ite) # gets everything\n",
    "\n",
    "print(\"Pattern '{}' matches {} files\".format(ALL_FILEPATTERN, len(all_filenames)))\n",
    "print(\"Pattern '{}' matches {} files\".format(EVAL_FILEPATTERN, len(eval_filenames)))\n",
    "print(\"Evaluation files: {}\".format(len(eval_filenames)))\n",
    "print(\"Training files: {}\".format(len(train_filenames)))\n",
    "print(\"Initial shape of the evaluation dataset: \" + str(evtemps.shape))\n",
    "print(\"{} files, {} data points per file, {} values per data point\"\n",
    "      \" (Tmin, Tmax, is_interpolated) \".format(evtemps.shape[0], evtemps.shape[1], evtemps.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innovative-dakota",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can adjust the visualisation range and dataset here\n",
    "# Interpolated regions of the dataset are marked in red\n",
    "WEATHER_STATION = 0 # 0 to 7 in default eval dataset\n",
    "START_DATE = 0 # 0 = Jan 2nd 1950\n",
    "END_DATE = 18262 # 18262 = Dec 31st 2009\n",
    "visu_temperatures = evtemps[WEATHER_STATION, START_DATE:END_DATE]\n",
    "visu_dates = evdates[START_DATE:END_DATE]\n",
    "\n",
    "utils_display.picture_this_4(visu_temperatures, visu_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-disorder",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-tucson",
   "metadata": {},
   "source": [
    "Our RNN would need to be unrolled across 365 steps to capture the yearly temperature cycles. That's a bit too much. We will resample the temperatures and work with 5-day averages for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time we ask the utility function to average temperatures over 5-day periods (RESAMPLE_BY=5)\n",
    "ite = utils_batching.rnn_multistation_sampling_tempereature_sequencer(eval_filenames, RESAMPLE_BY, tminmax=True)\n",
    "evaltemps, _, evaldates, _, _ = next(ite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display five years worth of data\n",
    "WEATHER_STATION = 0\n",
    "START_DATE = 0 \n",
    "END_DATE = 365*5//RESAMPLE_BY # 5 years\n",
    "visu_temperatures = evaltemps[WEATHER_STATION, START_DATE:END_DATE]\n",
    "visu_dates = evaldates[START_DATE:END_DATE]\n",
    "plt.fill_between(visu_dates, visu_temperatures[:,0], visu_temperatures[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-senator",
   "metadata": {},
   "source": [
    "## Visualise training sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-circular",
   "metadata": {},
   "source": [
    "This is what the neural network will see during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-maker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function rnn_multistation_sampling_temperature_sequencer puts one weather station per line in\n",
    "# a batch and continues with data from the same station in corresponding lines in the next batch.\n",
    "# Features and labels are returned with shapes [BATCHSIZE, SEQLEN, 2]. The last dimension of size 2\n",
    "# contains (Tmin, Tmax)\n",
    "ite = utils_batching.rnn_multistation_sampling_temperature_sequencer(eval_filenames,\n",
    "                                                                     RESAMPLE_BY,\n",
    "                                                                     BATCHSIZE,\n",
    "                                                                     SEQLEN,\n",
    "                                                                     N_FORWARD,\n",
    "                                                                     nb_epochs=1,\n",
    "                                                                     tminmax=True)\n",
    "\n",
    "# load 6 training sequences (each one contains data for all weather stations)\n",
    "visu_data = [next(ite) for _ in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that consecutive training sequences from the same weather station are indeed consecutive\n",
    "WEATHER_STATION = 4\n",
    "utils_display.picture_this_5(visu_data, WEATHER_STATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-extent",
   "metadata": {},
   "source": [
    "## The model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-jacob",
   "metadata": {},
   "source": [
    "<center><img src=\"RNN2.svg\" width=\"700px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-briefs",
   "metadata": {},
   "source": [
    "X shape [BATCHSIZE, SEQLEN, 2]\n",
    "\n",
    "Y shape [BATCHSIZE, SEQLEN, 2]\n",
    "\n",
    "H shape [BATCHSIZE, RNN_CELLSIZE*NLAYERS]\n",
    "\n",
    "When executed, this function instantiates the TensorFlow graph for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_rnn_fn(features, Hin, labels, step, dropout_pkeep):\n",
    "    X = features # shape [BATCHSIZE, SEQLEN, 2], 2 for (Tmin, Tmax)\n",
    "    batchsize = tf.shape(X)[0]\n",
    "    seqlen = tf.shape(X)[1]\n",
    "    pairlen = tf.shape(X)[2] # should be 2 (tmin, tmax)\n",
    "    \n",
    "    cells = [tf.nn.rnn_cell.GRUCell(RNN_CELLSIZE, activation=ACTIVATION) for _ in range(N_LAYERS)]\n",
    "    # dropout useful between cell layers only: no output dropout on last cell\n",
    "    cells = [tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout_pkeep) for cell in cells]\n",
    "    # a stacked RNN cell still works like an RNN cell\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=False)\n",
    "    # X[BATCHSIZE, SEQLEN, 2], Hin[BATCHSIZE, RNN_CELLSIZE*N_LAYERS]\n",
    "    # the sequence unrolling happens here\n",
    "    Yn, H = tf.nn.dynamic_rnn(cell, X, initial_state=Hin, dtype=tf.float32)\n",
    "    # Yn[BATCHSIZE, SEQLEN, RNN_CELLSIZE]\n",
    "    Yn = tf.reshape(Yn, [batchsize*seqlen, RNN_CELLSIZE])\n",
    "    Yr = tf.layers.dense(Yn, 2) # Yr[BATCHSIZE*SEQLEN, 2]\n",
    "    Yr = tf.reshape(Yr, [batchsize, seqlen, 2]) # Yr [BATCHSIZE, SEQLEN, 2]\n",
    "    Yout = Yr[:,-N_FORWARD:,:] # Last N_FORWARD outputs Yout[BATCHSIZE, N_FORWARD, 2]\n",
    "    \n",
    "    loss = tf.losses.mean_squared_error(Yr, labels) # labels[BATCHSIZE, SEQLEN, 2]\n",
    "    \n",
    "    lr = 0.001 + tf.train.exponential_decay(0.01, step, 1000, 0.5)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    train_op = optimizer.minmize(loss)\n",
    "    \n",
    "    return Yout, H, loss, train_op, Yr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-trance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-powell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-luther",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-truth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-chinese",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-sheep",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-communist",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-billion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-scoop",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-network",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
