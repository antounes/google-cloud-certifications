{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "metallic-policy",
   "metadata": {},
   "source": [
    "# Text generation using tensor2tensor on Cloud AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-convenience",
   "metadata": {},
   "source": [
    "This notebook illustrates the use of [tensor2tensor](https://github.com/tensorflow/tensor2tensor) library to do from-scratch, distributed training of a training model. Then, the trained model is used to complete new poems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-johnston",
   "metadata": {},
   "source": [
    "## Install tensor2tensor, and specify Google Cloud Platform project and bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-japanese",
   "metadata": {},
   "source": [
    "Install the necessary packages. tensor2tensor will give us the Transformer model. Project Gutenberg gives us access to historical poems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shaped-thousand",
   "metadata": {},
   "source": [
    "**p.s.** Note that this notebook uses Python 2 because Project Gutenberg relies on BSD-DB which was deprecated in Python 3 and removed from the standard library. tensor2tensor itself can be used on Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip freeze | grep tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install tensor2tensor=1.13.1 tensorflow=1.13.1 tensorflow-serving-api=1.13 gutenberg\n",
    "pip install tensorflow_hub\n",
    "\n",
    "# install from sou\n",
    "#git clone https://github.com/tensorflow/tensor2tensor.git\n",
    "#cd tensor2tensor\n",
    "#yes | pip install --user -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-cursor",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip freeze | grep tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-porter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = \"cloud-training-demos\" # Replace with your Project ID\n",
    "BUCKET = \"cloud-training-demos-ml\" # Replace with your bucket name\n",
    "REGION = \"us-central1\" # Replace with your bucket region\n",
    "\n",
    "# this is what this notebook is demonstrating\n",
    "PROBLEM = \"poetry_line_problem\"\n",
    "\n",
    "# for bash\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"PROBLEM\"] = PROBLEM\n",
    "\n",
    "# os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":\" + os.getcwd() + \"/tensor2tensor/tensor2tensor/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-domestic",
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-recorder",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-invasion",
   "metadata": {},
   "source": [
    "We will get some [poetry anthologies](https://www.gutenberg.org/wiki/Poetry_(Bookshelf)) from Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf data/poetry\n",
    "mkdir -p data/poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.cleanup import strip_headers\n",
    "import re\n",
    "\n",
    "books = [\n",
    "    # bookid, skip N lines, title\n",
    "    (26715, 1000, \"Victorian songs\"),\n",
    "    (30235, 580, \"Baldwin collection\"),\n",
    "    (35402, 710, \"Swinburne collection\"),\n",
    "    (574, 15, \"Blake\"),\n",
    "    (1304, 172, \"Bulchevys collection\"),\n",
    "    (19221, 223, \"Palgrave-Pearse collection\"),\n",
    "    (15553, 522, \"Knowles collection\")\n",
    "]\n",
    "\n",
    "with open(\"data/poetry/raw.txt\", \"w\") as ofp:\n",
    "    lineno = 0\n",
    "    for (id_nr, toskip, title) in books:\n",
    "        startline = lineno\n",
    "        text = strip_headers(load_etext(id_nr)).strip()\n",
    "        lines = text.split(\"\\n\")[toskip:]\n",
    "        # any line that is all upper case is a title or author name\n",
    "        # also don't want any lines with years (numbers)\n",
    "        for line in lines:\n",
    "            if (len(line) > 0\n",
    "                and line.upper() != line\n",
    "                and not re.match(\".*[0-9]+.*\", line)\n",
    "                and len(line) < 50\n",
    "                ):\n",
    "                cleaned = re.sub(\"[^a-z\\\"\\-]+\", \" \", line.strip().lower())\n",
    "                ofp.write(cleaned)\n",
    "                ofp.write(\"\\n\")\n",
    "                lineno += 1\n",
    "            else:\n",
    "                ofp.write(\"\\n\")\n",
    "        print(\"Wrote lines {} to {} from {}\".format(startline, lineno, title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l data/poetry/*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swiss-amount",
   "metadata": {},
   "source": [
    "## Create training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-product",
   "metadata": {},
   "source": [
    "We are going to train an ML model to write poetry given a starting point. We'll give it one line, and it's going to tell us the next line. So, naturally, we will train it on real poetry. Our feature will be a line of a poem and the label will be next line of that poem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-academy",
   "metadata": {},
   "source": [
    "Our training dataset will consist of two files. The first file will consist of the input lines of poetry and the other file will consist of the corresponding output lines, one output line per input line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/poetry/raw.txt\", \"r\") as rawfp,\\\n",
    "  open(\"data/poetry/input.txt\", \"w\") as infp,\\\n",
    "  open(\"data/poetry/output.txt\", \"w\") as outfp:\n",
    "    \n",
    "    prev_line = \"\"\n",
    "    for curr_line in rawfp:\n",
    "        curr_line = curr_line.strip()\n",
    "        # poems break at empty lines, so this ensures we train only on lines of the same poem\n",
    "        if len(prev_line) > 0 and len(curr_line) > 0:\n",
    "            infp.write(prev_line + \"\\n\")\n",
    "            outfp.write(curr_line + \"\\n\")\n",
    "        prev_line = curr_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-showcase",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -5 data/poetry/*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considered-trainer",
   "metadata": {},
   "source": [
    "We do not need to generate the data beforehand $-$ instead, we can have tensor2tensor create the training dataset for us. So, in the code below, we'll use only `data/poetry/raw.txt` $-$ obviously, this allows us to productionise our model better. Simply keep collecting raw data and generate the training/test data at the time of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-ready",
   "metadata": {},
   "source": [
    "## Set up problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-tackle",
   "metadata": {},
   "source": [
    "The Problem in tensor2tensor is where you specify parameters like the size of your vocabulary and where to get the training data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf poetry\n",
    "mkdir -p poetry/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile poetry/trainer/problem.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.models import transformer\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "from tensor2tensor.data_generators import generator_utils\n",
    "\n",
    "tf.summary.FileWriteCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "\n",
    "@registry.register_problem\n",
    "class PoetryLineProblem(text_problems.Text2TextProblem):\n",
    "    \"\"\"Predict next line of poetry from the last line from Gutenberg texts.\n",
    "    \"\"\"\n",
    "    \n",
    "    @property\n",
    "    def approx_vocab_size(self):\n",
    "        return 2**13 # ~8k\n",
    "    \n",
    "    @property\n",
    "    def is_generate_per_split(self):\n",
    "        # generate data will NOT shard the data into TRAIN and EVAL for us\n",
    "        return False\n",
    "    \n",
    "    @property\n",
    "    def dataset_splits(self):\n",
    "        \"\"\"Splits of data to produce and number of output shards for each.\n",
    "        \"\"\"\n",
    "        # 10% evaluation data\n",
    "        return [{\n",
    "            \"split\": problem.DatasetSplit.TRAIN,\n",
    "            \"shards\": 90,\n",
    "        }, {\n",
    "            \"split\": problem.DatasetSplit.Eval,\n",
    "            \"shards\": 10\n",
    "        }]\n",
    "    \n",
    "    def generate_samples(self, data_dir, tmp_dir, dataset_split):\n",
    "        with open(\"data/poetry/raw.txt\", \"r\") as rawfp:\n",
    "            prev_line = \"\"\n",
    "            for curr_line in rawfp:\n",
    "                curr_line = curr_line.strip()\n",
    "            # poems break at empty lines, so this ensures we train only on lines of the same poem\n",
    "            if len(prev_line) > 0 and len(curr_line) > 0:\n",
    "                yield {\n",
    "                    \"inputs\": prev_line,\n",
    "                    \"targets\": curr_line\n",
    "                }\n",
    "            prev_line = curr_line\n",
    "            \n",
    "# Smaller than the typical translate model, and with more regularisation\n",
    "@registry.register_hparams\n",
    "def transformer_poetry():\n",
    "    hparams = transformer.transformer_base()\n",
    "    hparams.num_hidden_layers = 2\n",
    "    hparams.hidden_size = 128\n",
    "    hparams.filter_size = 512\n",
    "    hparams.num_heads = 4\n",
    "    hparams.attention_dropout = 0.6\n",
    "    hparams.layer_prepostprocess_dropout = 0.6\n",
    "    hparams.learning_rate = 0.05\n",
    "    return hparams\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_poetry_tpu():\n",
    "    hparams = transformer_poetry()\n",
    "    transformer.update_hparams_for_tpu(hparams)\n",
    "    return hparams\n",
    "\n",
    "# hyperparameter tuning ranges\n",
    "@registry.register_ranged_hparams\n",
    "def transformer_poetry_range(rhp):\n",
    "    rhp.set_float(\"learning_rate\", 0.05, 0.25, scale=rhp.LOG_SCALE)\n",
    "    rhp.set_int(\"num_hidden_layers\", 2, 4)\n",
    "    rhp.set_discrete(\"hidden_size\", [128, 256, 512])\n",
    "    rhp.set_float(\"attention_dropout\", 0.4, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-virtue",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile poetry/trainer/__init__.py\n",
    "from . import problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-donor",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile poetry/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    \"tensor2tensor\"\n",
    "]\n",
    "\n",
    "setup(\n",
    "    name=\"poetry\",\n",
    "    version=\"0.1\",\n",
    "    author=\"Google\",\n",
    "    author_email=\"training-feedback@cloud.google.com\",\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description=\"Poetry Line Problem\",\n",
    "    requires=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch poetry/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-bloom",
   "metadata": {},
   "source": [
    "## Generate training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-access",
   "metadata": {},
   "source": [
    "Our problem (translation) requires the creation of text sequences from the training dataset. This is done using tensor2tensor-datagen and the Problem defined in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-spread",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "DATA_DIR=./t2t_data\n",
    "TMP_DIR=$DATA_DIR/tmp\n",
    "rm -rf $DATA_DIR $TMP_DIR\n",
    "mkdir -p $DATA_DIR $TMP_DIR\n",
    "# Generate data\n",
    "t2t-datagen \\\n",
    "    --t2t_usr_dir=./poetry/trainer \\\n",
    "    --problem=$PROBLEM \\\n",
    "    --data_dir=$DATA_DIR \\\n",
    "    --tmp_dir=$TMP_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-comfort",
   "metadata": {},
   "source": [
    "Let's check to see the files that were output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-dress",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls t2t_data | head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-cherry",
   "metadata": {},
   "source": [
    "## Provide Cloud AI Platform access to data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quick-aside",
   "metadata": {},
   "source": [
    "Copy the data to Google Cloud Storage, and then provide access to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-renewal",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "DATA_DIR=./t2t_data\n",
    "gsutil -m rm -r gs://${BUCKET}/poetry/\n",
    "gsutil -m cp ${DATA_DIR}/${PROBLEM}* ${DATA_DIR}/vocab* gs://${BUCKET}/poetry/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-ecuador",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "PROJECT_ID=$PROJECT\n",
    "AUTH_TOKEN=$(gcloud auth print-access-token)\n",
    "SVC_ACCOUNT=$(curl -X GET -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n",
    "    https://ml.googleapis.com/v1/projects/${PROJECT_ID}:getConfig \\\n",
    "    | python -c \"import json; import sys; response = json.load(sys.stdin) \\\n",
    "    print(response['serviceAccount'])\")\n",
    "\n",
    "echo \"Authorizing the Cloud AI Platform Service account $SVC_ACCOUNT to access files in $BUCKET\"\n",
    "gsutil -m defacl ch -u $SVC_ACCOUNT:R gs://$BUCKET\n",
    "gsutil -m acl ch -u $SVC_ACOUNT:R -r gs://$BUCKET\n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:W gs://$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-memorabilia",
   "metadata": {},
   "source": [
    "## Train model locally on subset of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-devices",
   "metadata": {},
   "source": [
    "Let's run it locally on a subset of the data to make sure it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-terry",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "BASE=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/subset\n",
    "gsutil -m rm -r $OUTDIR\n",
    "gsutil -m cp \\\n",
    "    ${BASE}/${PROBLEM}-train-0008* \\\n",
    "    ${BASE}/${PROBLEM}-dev-00000*  \\\n",
    "    ${BASE}/vocab* \\\n",
    "    $OUTDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-start",
   "metadata": {},
   "source": [
    "Note: the following will work only if you are running Jupyter on a reasonably powerful machine. Don't be alarmed if your process is killed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "DATA_DIR=$gs://${BUCKET}/poetry/subset\n",
    "OUTDIR=./trained_model\n",
    "rm -rf $OUTDIR\n",
    "t2t-trainer \\\n",
    "    --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "    --t2t_usr_dir=./poetry/trainer \\\n",
    "    --problem=$PROBLEM \\\n",
    "    --model=transformer \\\n",
    "    --hparams_set=transformer_poetry \\\n",
    "    --output_dir=$OUTDIR --job-dir=$OUTDIR --train_steps=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-native",
   "metadata": {},
   "source": [
    "## Option 1: Train model locally on full dataset (use if running on Notebook instance with a GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-athens",
   "metadata": {},
   "source": [
    "You can train on the full dataset if you are on a Google Cloud Notebook Instance with a P100 or better GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-football",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "LOCALGPU=\"--train_steps=7500 --worker_gpu=1 --hparams_set=transformer_poetry\"\n",
    "\n",
    "DATA_DIR=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/model\n",
    "rm -rf $OUTDIR\n",
    "t2t-trainer \\\n",
    "    --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "    --t2t_usr_dir=./poetry/trainer \\\n",
    "    --problem=$PROBLEM \\\n",
    "    --model=transformer \\\n",
    "    --hparams_set=transformer_poetry \\\n",
    "    --output_dir=$OUTDIR ${LOCAL_GPU}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signal-terminology",
   "metadata": {},
   "source": [
    "## Option 2: Train on Cloud AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-arbitration",
   "metadata": {},
   "source": [
    "tensor2tensor has a convenient `--cloud_mlengine` option to kick off the training on the managed service. It uses the Cloud AI Platform Python API, rather than requiring you to use gcloud to submit the job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-technician",
   "metadata": {},
   "source": [
    "Note: your project needs P100 quota in the region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "GPU=\"--train_steps=7500 --cloud_mlengine --worker_gpu=1 --hparams_set=tranformer_poetry\"\n",
    "\n",
    "DATADIR=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/model\n",
    "JOBNAME=poetry_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "echo \"'Y'\" | t2t-trainer \\\n",
    "    --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "    --t2t_usr_dir=./poetry/trainer \\\n",
    "    --problem=$PROBLEM \\\n",
    "    --model=transformer \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    ${GPU}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-hanging",
   "metadata": {},
   "source": [
    "## Option 3: Train on a directly-connected TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-springer",
   "metadata": {},
   "source": [
    "If you are running on a VM directly connected to a Cloud TPU, you can run t2t-trainer directly. Unfortunately, you won't see any output from Jupyter while the program is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-trader",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "TPU=\"--train_steps=7500 --use_tpu=True --cloud_tpu_name=laktpu --hparams_set=transformer_poetry_tpu\"\n",
    "\n",
    "DATADIR=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/model_tpu\n",
    "JOBNAME=poetry_$(date -y +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "echo \"'Y'\" | t2t-trainer \\\n",
    "    --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "    --t2t_usr_dir=./poetry/trainer \\\n",
    "    --problem=$PROBLEM \\\n",
    "    --model=transformer \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    ${TPU}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/poetry/model_tpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-means",
   "metadata": {},
   "source": [
    "## Option 4: Training longer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-gasoline",
   "metadata": {},
   "source": [
    "Let's train on 4 GPUs for 75,000 steps. Note the change in the last line of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "DATADIR=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/model_full2\n",
    "JOBNAME=poetry_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "echo \"'Y'\" | t2t-trainer \\\n",
    "    --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "    --t2t_usr_dir=./poetry/trainer \\\n",
    "    --problem=$PROBLEM \\\n",
    "    --model=transformer \\\n",
    "    --hparams_set=transformer_poetry \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    --train_steps=75000 --cloud_mlengine --worker_gpu=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-remedy",
   "metadata": {},
   "source": [
    "In orde that you have your expectations set correctly: a high-performing translation model needs 400-million lines of input and takes 1 whole day on a TPU pod!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-violation",
   "metadata": {},
   "source": [
    "## Check trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-begin",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gsutil ls gs://${BUCKET}/poetry/model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-hands",
   "metadata": {},
   "source": [
    "## Batch predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-jungle",
   "metadata": {},
   "source": [
    "How will your poetry model do when faced with Rumi's spiritual couplets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-collectible",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data/poetry/rumi.txt\n",
    "Where did the handsome beloved go?\n",
    "I wonder, where did that tall, shapely cypress tree go?\n",
    "He spread his light among us like a candle.\n",
    "Where did he go? So strange, where did he go without me?\n",
    "All day long my heart trembles like a leaf.\n",
    "All alone at midnight, where did that beloved go?\n",
    "Go to the road, and ask any passing traveler — \n",
    "That soul-stirring companion, where did he go?\n",
    "Go to the garden, and ask the gardener — \n",
    "That tall, shapely rose stem, where did he go?\n",
    "Go to the rooftop, and ask the watchman — \n",
    "That unique sultan, where did he go?\n",
    "Like a madman, I search in the meadows!\n",
    "That deer in the meadows, where did he go?\n",
    "My tearful eyes overflow like a river — \n",
    "That pearl in the vast sea, where did he go?\n",
    "All night long, I implore both moon and Venus — \n",
    "That lovely face, like a moon, where did he go?\n",
    "If he is mine, why is he with others?\n",
    "Since he’s not here, to what “there” did he go?\n",
    "If his heart and soul are joined with God,\n",
    "And he left this realm of earth and water, where did he go?\n",
    "Tell me clearly, Shams of Tabriz,\n",
    "Of whom it is said, “The sun never dies” — where did he go?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-practitioner",
   "metadata": {},
   "source": [
    "Let's write out the odd-numbered lines. We'll compare how close our model can get to the beauty of Rumi's second lines given his first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "awk \"NR % 2 == 1\" data/poetry/rumitxt | tr \"[:upper:]\" \"[:lower:]\" | sed \"s/[^a-z\\'-\\ ]//g\" > data/poetry/rumi_leads.txt\n",
    "head -3 data/poetry/rumi_leads.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "TOPDIR=gs://${BUCKET}\n",
    "OUTDIR=${TOPDIR}/poetry/model\n",
    "DATADIR=${TOPDIR}/poetry/data\n",
    "MODEL=transformer\n",
    "HPARAMS=transformer_poetry\n",
    "\n",
    "# The file with the input lines\n",
    "DECODE_FILE=data/poetry/rumi_leads.txt\n",
    "\n",
    "BEAM_SIZE=4\n",
    "ALPHA=0.6\n",
    "\n",
    "t2t-decoder \\\n",
    "    --data_dir=$DATADIR \\\n",
    "    --problem=$PROBLEM \\\n",
    "    --model=$MODEL \\\n",
    "    --hparams_set=$HPARAMS \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    --t2t_usr_dir=./poetry/trainer \\\n",
    "    --decode_hparams=\"beam_size=$BEAM_SIZE, alpha=$ALPHA\" \\\n",
    "    --decode_from_file=$DECODE_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "DECODE_FILE=data/poetry/rumi_leads.txt\n",
    "cat ${DECODE_FILE}.*.decodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-supplement",
   "metadata": {},
   "source": [
    "Some of these are still phrases and not complete sentences. This indicates that we might need to train longer or better somehow. We need to diagnose the model..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-examination",
   "metadata": {},
   "source": [
    "## Diagnosing training run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empty-sequence",
   "metadata": {},
   "source": [
    "Let's diagnose the training run to see what we'd improve the next time around. (Note that this package may not be present on Jupyter `--pip install pydatalab` if necessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-belize",
   "metadata": {},
   "source": [
    "**Monitor training with TensorBoard**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-marker",
   "metadata": {},
   "source": [
    "To activate TensorBoard within the JupyterLab UI, navigate to **\"File\" - \"New Launcher\"**. Then double-click the \"TensorBoard\" icon on the bottom row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equal-knife",
   "metadata": {},
   "source": [
    "TensorBoard 1 will appear in the new tab. Navigate through the three tabs to see the active TensorBoard. The \"Graphs\" and \"Projector\" tabs offer very interesting information including the ability to replay the tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-watershed",
   "metadata": {},
   "source": [
    "You may close the TensorBoard tab when you are finished exploring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-graduate",
   "metadata": {},
   "source": [
    "We need to reduce overfitting and make sure the eval metrics keep going down as long as the loss is also going down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-northeast",
   "metadata": {},
   "source": [
    "What we really need is to get more data, but if that's not an option, we could try to reduce the Neural Network and increase the dropout regularisation. We could also do hyperparameter tuning on the dropout and network sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heavy-active",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "present-better",
   "metadata": {},
   "source": [
    "tensor2tensor also supports hyperparameter tuning on Cloud AI Platform. Note the addition of autotune flags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stainless-musical",
   "metadata": {},
   "source": [
    "The `transformer_poetry_range` was registered in `problem.py` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-reverse",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "DATADIR=gs://${BUCKET}/poetry/data\n",
    "OUTDIR=gs://${BUCKET}/poetry/model_hparam\n",
    "JOBNAME=poetry_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "echo \"'Y'\" | t2t-trainer \\\n",
    "    --data_dir=gs://${BUCKET}/poetry/subset \\\n",
    "    --t2t_usr_dir=./poetry/trainer \\\n",
    "    --problem=$PROBLEM \\\n",
    "    --model=transformer \\\n",
    "    --hparams_set=transformer_poetry \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    --haprams_range=transformer_poetry_range \\\n",
    "    --autotune_objective=\"metrics-poetry_line_problem/accuracy_per_sequence\" \\\n",
    "    --autotune_maximize \\\n",
    "    --autotune_max_trials=4 \\\n",
    "    --autotune_parallel_trials=4 \\\n",
    "    --train_steps=7500 --cloud_mlengine --worker_gpu=4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-defense",
   "metadata": {},
   "source": [
    "Let's try predicting with this optimised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "major-mauritius",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "BEST_TRIAL = xx # change as needed\n",
    "TOPDIR=gs://${BUCKET}\n",
    "OUTDIR=${TOPDIR}/poetry/model_hparam/$BEST_TRIAL\n",
    "DATADIR=${TOPDIR}/poetry/data\n",
    "MODEL=transformer\n",
    "HPARAMS=transformer_poetry\n",
    "\n",
    "# the file with the input lines\n",
    "DECODE_FILE=data/poetry/rumi_leads.txt\n",
    "\n",
    "BEAM_SIZE=4\n",
    "ALPHA=0.6\n",
    "\n",
    "t2t-decoder \\\n",
    "    --data_dir=$DATADIR \\\n",
    "    --problem=$PROBLEM \\\n",
    "    --model=$MODEL \\\n",
    "    --hparams_set=$HPARAMS \\\n",
    "    --output_dir=$OUTDIR \\\n",
    "    --t2t_usr_dir=./poetry/trainer \\\n",
    "    --decode_hparams=\"beam_size=$BEAM_SIZE,alpha=$ALPHA\" \\\n",
    "    --decode_from_file=$DECODE_FILE \\\n",
    "    --hparams=\"num_hidden_layers=4,hidden_size=512\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "DECODE_FILE=data/poetry/rumi_leads.txt\n",
    "cat ${DECODE_FILE}.*.decodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-legislature",
   "metadata": {},
   "source": [
    "## Serving model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-adoption",
   "metadata": {},
   "source": [
    "There are two ways of serving predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-morning",
   "metadata": {},
   "source": [
    "1. Use Cloud AI Platform $-$ this is serverless and you don't have to manage any infrastructure\n",
    "2. Use Kubeflow on Google Kubernetes Engine $-$ this uses clusters but will also work on-prem on your own Kubernetes cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-disposal",
   "metadata": {},
   "source": [
    "In either case, you need to export the model first and have TensorFlow serving serve the model. The model, however, expects to see *encoded* (i.e. preprocessed) data. So, we'll do that in the Python Flask application (in AppEngine Flex) that serves the user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "TOPDIR=gs://${BUCKET}\n",
    "OUTDIR=${TOPDIR}/poetry/model_full2\n",
    "DATADIR=${TOPDIR}/poetry/data\n",
    "MODEL=transformer\n",
    "HPARAMS=transformer_poetry\n",
    "BEAM_SIZE=4\n",
    "ALPHA=0.6\n",
    "\n",
    "t2t-exporter \\\n",
    "    --model=$MODEL \\\n",
    "    --hparams_set=$HPARAMS \\\n",
    "    --problem=$PROBLEM \\\n",
    "    --t2t_usr_dir=./poetry/trainer \\\n",
    "    --decode_hparams=\"beam_size=$BEAM_SIZE,alpha=$ALPHA\" \\\n",
    "    --data_dir=$DATADIR \\\n",
    "    --output_dir=$OUTDIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/poetry/model_full2/export | tail -1)\n",
    "echo $MODEL_LOCATION\n",
    "sved_model_cli show --dir $MODEL_LOCATION --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-century",
   "metadata": {},
   "source": [
    "### Cloud AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-being",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mlengine.json\n",
    "description: Poetry service on AI Platform\n",
    "autoScaling:\n",
    "    minNodes: 1 # We don't want this model to autoscale down to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_NAME=\"poetry\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "MODEL_LOCATION=$(gsutil ls gs://${BUCKET}/poetry/model_full2/export | tail -1)\n",
    "echo \"Deleting and deploying $MODEL_NAME $MODEL_VERSION from $MODEL_LOCATION ... this will take a few minutes\"\n",
    "gcloud ml-engine versions delete ${MODEL_VERSION} --model ${MODEL_NAME}\n",
    "#gcloud ml-engine models delete ${MODEL_NAME}\n",
    "#gcloud ml-engine models create ${MODEL_NAME} --regions $REGION\n",
    "gcloud ml-engine versions create ${MODEL_VERSION} \\\n",
    "        --model ${MODEL_NAME} --origin ${MODEL_LOCATION} --runtime-version=1.13 --config=mlengine.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-sudan",
   "metadata": {},
   "source": [
    "### Kubeflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-couple",
   "metadata": {},
   "source": [
    "Follow these instructions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-performance",
   "metadata": {},
   "source": [
    "- On the GCP console, launch a Google Kubernetes Engine (GKE) cluster named `poetry` with **2 nodes**, each of which is a **n1-standard-2** (2 vCPUs, 7.5 GB memory) VM\n",
    "- On the GCP console, click on the Connect button for your cluster, and choose the Cloud Shell option\n",
    "- In Cloud Shell, run:\n",
    "\n",
    "    `git clone https://github.com/GoogleCloudPlatform/training-data-analyst`\n",
    "    \n",
    "    `cd training-data-analyst/courses/machine_learning/deepdive/09_sequence`\n",
    "- Look at `./setup_kubeflow.sh` and modify as appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-tenant",
   "metadata": {},
   "source": [
    "### AppEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-drinking",
   "metadata": {},
   "source": [
    "What's deployed in Cloud AI Platform or Kubeflow is only the TensorFlow model. We still need a preprocessing service. That is done using AppEngine. Edit `application/app.yaml` appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proud-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat application/app.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-evaluation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd application\n",
    "#gcloud app create # if this is your first app\n",
    "#gcloud app deploy --quiet --stop-previous-version app.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspended-ivory",
   "metadata": {},
   "source": [
    "Now visit https://mlpoetry-dot-cloud-training-demos.appspot.com and try out the prediction app!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
