{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "treated-graduate",
   "metadata": {},
   "source": [
    "# Text generation using tensor2tensor on Cloud AI Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "israeli-surface",
   "metadata": {},
   "source": [
    "This notebook illustrates the use of [tensor2tensor](https://github.com/tensorflow/tensor2tensor) library to do from-scratch, distributed training of a training model. Then, the trained model is used to complete new poems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-burke",
   "metadata": {},
   "source": [
    "## Install tensor2tensor, and specify Google Cloud Platform project and bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-layout",
   "metadata": {},
   "source": [
    "Install the necessary packages. tensor2tensor will give us the Transformer model. Project Gutenberg gives us access to historical poems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-buddy",
   "metadata": {},
   "source": [
    "**p.s.** Note that this notebook uses Python 2 because Project Gutenberg relies on BSD-DB which was deprecated in Python 3 and removed from the standard library. tensor2tensor itself can be used on Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-guinea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip freeze | grep tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install tensor2tensor=1.13.1 tensorflow=1.13.1 tensorflow-serving-api=1.13 gutenberg\n",
    "pip install tensorflow_hub\n",
    "\n",
    "# install from sou\n",
    "#git clone https://github.com/tensorflow/tensor2tensor.git\n",
    "#cd tensor2tensor\n",
    "#yes | pip install --user -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip freeze | grep tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = \"cloud-training-demos\" # Replace with your Project ID\n",
    "BUCKET = \"cloud-training-demos-ml\" # Replace with your bucket name\n",
    "REGION = \"us-central1\" # Replace with your bucket region\n",
    "\n",
    "# this is what this notebook is demonstrating\n",
    "PROBLEM = \"poetry_line_problem\"\n",
    "\n",
    "# for bash\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"BUCKET\"] = BUCKET\n",
    "os.environ[\"REGION\"] = REGION\n",
    "os.environ[\"PROBLEM\"] = PROBLEM\n",
    "\n",
    "# os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":\" + os.getcwd() + \"/tensor2tensor/tensor2tensor/bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-hardware",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-kingdom",
   "metadata": {},
   "source": [
    "We will get some [poetry anthologies](https://www.gutenberg.org/wiki/Poetry_(Bookshelf)) from Project Gutenberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-poetry",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf data/poetry\n",
    "mkdir -p data/poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.cleanup import strip_headers\n",
    "import re\n",
    "\n",
    "books = [\n",
    "    # bookid, skip N lines, title\n",
    "    (26715, 1000, \"Victorian songs\"),\n",
    "    (30235, 580, \"Baldwin collection\"),\n",
    "    (35402, 710, \"Swinburne collection\"),\n",
    "    (574, 15, \"Blake\"),\n",
    "    (1304, 172, \"Bulchevys collection\"),\n",
    "    (19221, 223, \"Palgrave-Pearse collection\"),\n",
    "    (15553, 522, \"Knowles collection\")\n",
    "]\n",
    "\n",
    "with open(\"data/poetry/raw.txt\", \"w\") as ofp:\n",
    "    lineno = 0\n",
    "    for (id_nr, toskip, title) in books:\n",
    "        startline = lineno\n",
    "        text = strip_headers(load_etext(id_nr)).strip()\n",
    "        lines = text.split(\"\\n\")[toskip:]\n",
    "        # any line that is all upper case is a title or author name\n",
    "        # also don't want any lines with years (numbers)\n",
    "        for line in lines:\n",
    "            if (len(line) > 0\n",
    "                and line.upper() != line\n",
    "                and not re.match(\".*[0-9]+.*\", line)\n",
    "                and len(line) < 50\n",
    "                ):\n",
    "                cleaned = re.sub(\"[^a-z\\\"\\-]+\", \" \", line.strip().lower())\n",
    "                ofp.write(cleaned)\n",
    "                ofp.write(\"\\n\")\n",
    "                lineno += 1\n",
    "            else:\n",
    "                ofp.write(\"\\n\")\n",
    "        print(\"Wrote lines {} to {} from {}\".format(startline, lineno, title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-testing",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l data/poetry/*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-foundation",
   "metadata": {},
   "source": [
    "## Create training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-railway",
   "metadata": {},
   "source": [
    "We are going to train an ML model to write poetry given a starting point. We'll give it one line, and it's going to tell us the next line. So, naturally, we will train it on real poetry. Our feature will be a line of a poem and the label will be next line of that poem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-coupon",
   "metadata": {},
   "source": [
    "Our training dataset will consist of two files. The first file will consist of the input lines of poetry and the other file will consist of the corresponding output lines, one output line per input line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/poetry/raw.txt\", \"r\") as rawfp,\\\n",
    "  open(\"data/poetry/input.txt\", \"w\") as infp,\\\n",
    "  open(\"data/poetry/output.txt\", \"w\") as outfp:\n",
    "    \n",
    "    prev_line = \"\"\n",
    "    for curr_line in rawfp:\n",
    "        curr_line = curr_line.strip()\n",
    "        # poems break at empty lines, so this ensures we train only on lines of the same poem\n",
    "        if len(prev_line) > 0 and len(curr_line) > 0:\n",
    "            infp.write(prev_line + \"\\n\")\n",
    "            outfp.write(curr_line + \"\\n\")\n",
    "        prev_line = curr_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -5 data/poetry/*.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-killer",
   "metadata": {},
   "source": [
    "We do not need to generate the data beforehand $-$ instead, we can have tensor2tensor create the training dataset for us. So, in the code below, we'll use only `data/poetry/raw.txt` $-$ obviously, this allows us to productionise our model better. Simply keep collecting raw data and generate the training/test data at the time of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-keeping",
   "metadata": {},
   "source": [
    "## Set up problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-involvement",
   "metadata": {},
   "source": [
    "The Problem in tensor2tensor is where you specify parameters like the size of your vocabulary and where to get the training data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf poetry\n",
    "mkdir -p poetry/trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-slide",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile poetry/trainer/problem.py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensor2tensor.utils import registry\n",
    "from tensor2tensor.models import transformer\n",
    "from tensor2tensor.data_generators import problem\n",
    "from tensor2tensor.data_generators import text_encoder\n",
    "from tensor2tensor.data_generators import text_problems\n",
    "from tensor2tensor.data_generators import generator_utils\n",
    "\n",
    "tf.summary.FileWriteCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "\n",
    "@registry.register_problem\n",
    "class PoetryLineProblem(text_problems.Text2TextProblem):\n",
    "    \"\"\"Predict next line of poetry from the last line from Gutenberg texts.\n",
    "    \"\"\"\n",
    "    \n",
    "    @property\n",
    "    def approx_vocab_size(self):\n",
    "        return 2**13 # ~8k\n",
    "    \n",
    "    @property\n",
    "    def is_generate_per_split(self):\n",
    "        # generate data will NOT shard the data into TRAIN and EVAL for us\n",
    "        return False\n",
    "    \n",
    "    @property\n",
    "    def dataset_splits(self):\n",
    "        \"\"\"Splits of data to produce and number of output shards for each.\n",
    "        \"\"\"\n",
    "        # 10% evaluation data\n",
    "        return [{\n",
    "            \"split\": problem.DatasetSplit.TRAIN,\n",
    "            \"shards\": 90,\n",
    "        }, {\n",
    "            \"split\": problem.DatasetSplit.Eval,\n",
    "            \"shards\": 10\n",
    "        }]\n",
    "    \n",
    "    def generate_samples(self, data_dir, tmp_dir, dataset_split):\n",
    "        with open(\"data/poetry/raw.txt\", \"r\") as rawfp:\n",
    "            prev_line = \"\"\n",
    "            for curr_line in rawfp:\n",
    "                curr_line = curr_line.strip()\n",
    "            # poems break at empty lines, so this ensures we train only on lines of the same poem\n",
    "            if len(prev_line) > 0 and len(curr_line) > 0:\n",
    "                yield {\n",
    "                    \"inputs\": prev_line,\n",
    "                    \"targets\": curr_line\n",
    "                }\n",
    "            prev_line = curr_line\n",
    "            \n",
    "# Smaller than the typical translate model, and with more regularisation\n",
    "@registry.register_hparams\n",
    "def transformer_poetry():\n",
    "    hparams = transformer.transformer_base()\n",
    "    hparams.num_hidden_layers = 2\n",
    "    hparams.hidden_size = 128\n",
    "    hparams.filter_size = 512\n",
    "    hparams.num_heads = 4\n",
    "    hparams.attention_dropout = 0.6\n",
    "    hparams.layer_prepostprocess_dropout = 0.6\n",
    "    hparams.learning_rate = 0.05\n",
    "    return hparams\n",
    "\n",
    "@registry.register_hparams\n",
    "def transformer_poetry_tpu():\n",
    "    hparams = transformer_poetry()\n",
    "    transformer.update_hparams_for_tpu(hparams)\n",
    "    return hparams\n",
    "\n",
    "# hyperparameter tuning ranges\n",
    "@registry.register_ranged_hparams\n",
    "def transformer_poetry_range(rhp):\n",
    "    rhp.set_float(\"learning_rate\", 0.05, 0.25, scale=rhp.LOG_SCALE)\n",
    "    rhp.set_int(\"num_hidden_layers\", 2, 4)\n",
    "    rhp.set_discrete(\"hidden_size\", [128, 256, 512])\n",
    "    rhp.set_float(\"attention_dropout\", 0.4, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile poetry/trainer/__init__.py\n",
    "from . import problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile poetry/setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    \"tensor2tensor\"\n",
    "]\n",
    "\n",
    "setup(\n",
    "    name=\"poetry\",\n",
    "    version=\"0.1\",\n",
    "    author=\"Google\",\n",
    "    author_email=\"training-feedback@cloud.google.com\",\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description=\"Poetry Line Problem\",\n",
    "    requires=[]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "!touch poetry/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-incident",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-thousand",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-sector",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-billion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-festival",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-saint",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-treatment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-temperature",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
